{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Capstone Project - Reinforcement Learning\n",
    "### Goal\n",
    "- Learn Object Oriented Programming techniques\n",
    "- Build your own Machine Learning model from scratch - Reinforcement Learning\n",
    "- Teach it to pickup and deliver items"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is Reinforcement Learning?\n",
    "- Reinforcement learning teaches the machine to think for itself based on past action rewards.\n",
    "![Reinforcement Learning](img/ReinforcementLearning.png)\n",
    "- Basically, the Reinforcement Learning algorithm tries to predict actions that gives rewards and avoids punishment.\n",
    "- It is like training a dog. You and the dog do not talk the same language, but the dogs learns how to act based on rewards (and punishment, which I do not advise or advocate).\n",
    "- Hence, if a dog is rewarded for a certain action in a given situation, then next time it is exposed to a similar situation it will act the same.\n",
    "- Translate that to Reinforcement Learning.\n",
    "    - The **agent** is the dog that is exposed to the **environment**.\n",
    "    - Then the **agent** encounters a **state**.\n",
    "    - The **agent** performs an **action** to transition to a **new state**.\n",
    "    - Then after the transition the **agent** receives a **reward** or **penalty (punishment)**.\n",
    "    - This forms a **policy** to create a strategy to choose actions in a given **state**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What algorithms are used for Reinforcement Learning?\n",
    "- The most common algorithm for Reinforcement Learning are.\n",
    "    - **Q-Learning**: is a model-free reinforcement learning algorithm to learn a policy telling an agent what action to take under what circumstances.\n",
    "    - **Temporal Difference**: refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function.\n",
    "    - **Deep Adversarial Network**: is a technique employed in the field of machine learning which attempts to fool models through malicious input.\n",
    "- We will focus on the **Q-learning** algorithm as it is easy to understand as well as powerful."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How does the Q-learning algorithm work?\n",
    "- As already noted, I just love this algorithm. It is “easy” to understand and powerful as you will see.\n",
    "![Q-Learning algorithm](img/QLearning.png)\n",
    "- The **Q-Learning** algorithm has a **Q-table** (a Matrix of dimension state x actions – don’t worry if you do not understand what a Matrix is, you will not need the mathematical aspects of it – it is just an indexed “container” with numbers).\n",
    "- The **agent** (or **Q-Learning** algorithm) will be in a state.\n",
    "- Then in each iteration the **agent** needs take an **action**.\n",
    "- The **agent** will continuously update the **reward** in the **Q-table**.\n",
    "- The **learning** can come from either **exploiting** or **exploring**.\n",
    "- This translates into the following pseudo algorithm for the **Q-Learning**.\n",
    "- The **agent** is in a given **stateºº and needs to choose an **action**.\n",
    "\n",
    "#### Algorithm\n",
    "- Initialise the **Q-table** to all zeros\n",
    "- Iterate\n",
    "    - Agent is in state **state**.\n",
    "    - With probability **epsilon** choose to **explore**, else **exploit**.\n",
    "        - If **explore**, then choose a *random* **action**.\n",
    "        - If **exploit**, then choose the *best* **action** based on the current **Q-table**.\n",
    "    - Update the **Q-table** from the new **reward** to the previous state.\n",
    "    - Q[**state, action**] = (1 – **alpha**) * Q[**state, action**] + **alpha** * (**reward + gamma** * max(Q[**new_state**]) — Q[**state, action**])\n",
    "    \n",
    "#### Variables\n",
    "As you can se, we have introduced the following variables.\n",
    "\n",
    "- **epsilon**: the probability to take a random action, which is done to explore new territory.\n",
    "- **alpha**: is the learning rate that the algorithm should make in each iteration and should be in the interval from 0 to 1.\n",
    "- **gamma**: is the discount factor used to balance the immediate and future reward. This value is usually between 0.8 and 0.99\n",
    "- **reward**: is the feedback on the action and can be any number. Negative is penalty (or punishment) and positive is a reward."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Description of task we want to solve\n",
    "- To keep it simple, we create a field of size 10×10 positions. In that field there is an item that needs to be picked up and moved to a drop-off point.\n",
    "![Field](img/Field-ReinforcementLearning.png)\n",
    "- At each position there are 6 different actions that can be taken.\n",
    "    - **Action 0**: Go South if on field.\n",
    "    - **Action 1**: Go North if on field.\n",
    "    - **Action 2**: Go East if on field (Please no).\n",
    "    - **Action 3**: Go West if on field.\n",
    "    - **Action 4**: Pickup item (it can try even if it is not there)\n",
    "    - **Action 5**: Drop-off item (it can try even if it does not have it)\n",
    "- Based on these actions we will make a reward system.\n",
    "    - If the **agent** tries to go off the **field**, punish with **-10** in **reward**.\n",
    "    - If the **agent** makes a (legal) move, punish with **-1** in **reward**, as we do not want to encourage endless walking around.\n",
    "    - If the **agent** tries to pick up item, but it is not there or it has it already, punish with **-10** in **reward**.\n",
    "    - If the **agent** picks up the item correct place, **reward** with **20**.\n",
    "    - If **agent** tries to drop-off item in wrong place or does not have the item, punish with **-10** in **reward**.\n",
    "    - If the **agent** drops-off item in correct place, **reward** with **20**.\n",
    "- That translates into the following code. I prefer to implement this code, as I think the standard libraries that provide similar frameworks hide some important details. As an example, and shown later, how do you map this into a state in the Q-table?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Field:\n",
    "    def __init__(self, size, item_pickup, item_drop_off, start_position):\n",
    "        self.size = size\n",
    "        self.item_pickup = item_pickup\n",
    "        self.item_drop_off = item_drop_off\n",
    "        self.position = start_position\n",
    "        self.item_in_car = False\n",
    "        \n",
    "    def get_number_of_states(self):\n",
    "        return self.size*self.size*self.size*self.size*2\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = self.position[0]*self.size*self.size*self.size*2\n",
    "        state = state + self.position[1]*self.size*self.size*2\n",
    "        state = state + self.item_pickup[0]*self.size*2\n",
    "        state = state + self.item_pickup[1]*2\n",
    "        if self.item_in_car:\n",
    "            state = state + 1\n",
    "        return state\n",
    "        \n",
    "    def make_action(self, action):\n",
    "        (x, y) = self.position\n",
    "        if action == 0:  # Go South\n",
    "            if y == self.size - 1:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x, y + 1)\n",
    "                return -1, False\n",
    "        elif action == 1:  # Go North\n",
    "            if y == 0:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x, y - 1)\n",
    "                return -1, False\n",
    "        elif action == 2:  # Go East\n",
    "            if x == 0:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x - 1, y)\n",
    "                return -1, False\n",
    "        elif action == 3:  # Go West\n",
    "            if x == self.size - 1:\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.position = (x + 1, y)\n",
    "                return -1, False\n",
    "        elif action == 4:  # Pickup item\n",
    "            if self.item_in_car:\n",
    "                return -10, False\n",
    "            elif self.item_pickup != (x, y):\n",
    "                return -10, False\n",
    "            else:\n",
    "                self.item_in_car = True\n",
    "                return 20, False\n",
    "        elif action == 5:  # Drop off item\n",
    "            if not self.item_in_car:\n",
    "                return -10, False\n",
    "            elif self.item_drop_off != (x, y):\n",
    "                self.item_pickup = (x, y)\n",
    "                self.item_in_car = False\n",
    "                return -10, False\n",
    "            else:\n",
    "                return 20, True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "field = Field(10, (0, 0), (9, 9), (9, 0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "field = Field(10, (0, 0), (9, 9), (9, 0))\n",
    "\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "field.make_action(2)\n",
    "\n",
    "field.make_action(4)\n",
    "\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "field.make_action(0)\n",
    "\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "field.make_action(3)\n",
    "\n",
    "field.make_action(5)\n",
    "\n",
    "field.position"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def naive_solution():\n",
    "    size = 10\n",
    "    item_start = (0, 0)\n",
    "    item_drop_off = (9, 9)\n",
    "    start_position = (0, 9)\n",
    "    \n",
    "    field = Field(size, item_start, item_drop_off, start_position)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = random.randint(0, 5)\n",
    "        reward, done = field.make_action(action)\n",
    "        steps = steps + 1\n",
    "    \n",
    "    return steps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "naive_solution()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "49845"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "runs = [naive_solution() for _ in range(100)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "sum (runs) / len(runs)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "135422.16"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "size = 10\n",
    "item_start = (0, 0)\n",
    "item_drop_off = (9, 9)\n",
    "start_position = (0, 9)\n",
    "\n",
    "field = Field(size, item_start, item_drop_off, start_position)\n",
    "\n",
    "number_of_states = field.get_number_of_states()\n",
    "number_of_actions = 6\n",
    "\n",
    "q_table = np.zeros((number_of_states, number_of_actions))\n",
    "\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "\n",
    "for _ in range(10000):\n",
    "    field = Field(size, item_start, item_drop_off, start_position)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        state = field.get_state()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 5)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "        reward, done = field.make_action(action)\n",
    "        # Q[state, action] = (1 – alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) — Q[state, action])\n",
    "        \n",
    "        new_state = field.get_state()\n",
    "        new_state_max = np.max(q_table[new_state])\n",
    "        \n",
    "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def reinforcement_learning():\n",
    "    epsilon = 0.1\n",
    "    alpha = 0.1\n",
    "    gamma = 0.6\n",
    "    \n",
    "    field = Field(size, item_start, item_drop_off, start_position)\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = field.get_state()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.randint(0, 5)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "        reward, done = field.make_action(action)\n",
    "        # Q[state, action] = (1 – alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]) — Q[state, action])\n",
    "        \n",
    "        new_state = field.get_state()\n",
    "        new_state_max = np.max(q_table[new_state])\n",
    "        \n",
    "        q_table[state, action] = (1 - alpha)*q_table[state, action] + alpha*(reward + gamma*new_state_max - q_table[state, action])\n",
    "        \n",
    "        steps = steps + 1\n",
    "    \n",
    "    return steps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "reinforcement_learning()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "runs_rl = [reinforcement_learning() for _ in range(100)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sum(runs_rl)/len(runs_rl)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('ML_DS': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "746800f96ece29ab8b5df47a779cb8f76655f7cf39bc7615a93fb17b55f50443"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}